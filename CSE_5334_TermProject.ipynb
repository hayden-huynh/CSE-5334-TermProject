{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1UJen3sXTYGILgXlQ6vzCBqWOlaNcNAtb",
      "authorship_tag": "ABX9TyMTq2CNERBeWKRSTXRGYkZG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hayden-huynh/CSE-5334-TermProject/blob/master/CSE_5334_TermProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CSE 5334 - Data Mining: Term Project"
      ],
      "metadata": {
        "id": "spl--tTaQIVz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## About The Project\n",
        "- Dataset: [Top Reddit Posts and Comments](https://www.kaggle.com/datasets/tushar5harma/topredditcomments?select=Top_Posts.csv)\n",
        "- Goal: Given a comment of a post, classify which subreddit (Machine Learning, Artificial Intelligence, or Data Science) that it belongs to\n",
        "- Solution: Naive Bayes Classifier for Text Classification"
      ],
      "metadata": {
        "id": "1uncsqW5QwC9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Steps\n",
        "1. Download the Top Reddit Posts and Comments [dataset](https://www.kaggle.com/datasets/tushar5harma/topredditcomments?select=Top_Posts.csv) from Kaggle\n",
        "2. Based on the original dataset, formulate a dataframe consisting of a column of **comment text**, and a column of subreddit class\n",
        "3. Perform text pre-processing\n",
        "  - Lower-casing\n",
        "  - Punctuation removal\n",
        "  - Tokenization and duplicate word removal\n",
        "4. Split the data into ***train*** (70%), ***dev*** (20%), and ***test*** (10%) subsets\n",
        "5. Train the Naive Bayes Classifier\n",
        "6. Experiment for optimization with **dev** dataset\n",
        "  - Smoothing\n",
        "  - Stopword removal\n",
        "  - Stemming / Lemmatization\n",
        "7. Conclude final accuracy with **test** dataset"
      ],
      "metadata": {
        "id": "tuZKsbctQNDX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the Dataset"
      ],
      "metadata": {
        "id": "9ryhN4-zRi99"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "4lJIzW-FP5OE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6535dbaf-867c-4f0b-997a-8c6a7d791f32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.13)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.27.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.65.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.26.15)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2022.12.7)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (2.0.12)\n",
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "topredditcomments.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  topredditcomments.zip\n",
            "replace Top_Posts.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: Top_Posts.csv           \n",
            "replace Top_Posts_Comments.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: Top_Posts_Comments.csv  \n"
          ]
        }
      ],
      "source": [
        "# Download the Top Reddit Posts & Reviews dataset from Kaggle\n",
        "# Reference 1 (Ref 1): https://www.analyticsvidhya.com/blog/2021/06/how-to-load-kaggle-datasets-directly-into-google-colab/\n",
        "\n",
        "# Ref 1 starts =====\n",
        "! pip install kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download tushar5harma/topredditcomments\n",
        "! unzip topredditcomments.zip\n",
        "# ===== Ref 1 ends"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Formulate Comment-Subreddit Dataframe"
      ],
      "metadata": {
        "id": "i03Qyh_9CakO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "top_posts = pd.read_csv(\"/content/Top_Posts.csv\")\n",
        "top_posts_comments = pd.read_csv(\"/content/Top_Posts_Comments.csv\")\n",
        "\n",
        "posts_id_class = top_posts[[\"post_id\", \"subreddit\"]]\n",
        "\n",
        "# Three classes: \"MachineLearning\", \"datascience\", \"artificial\"\n",
        "comments = pd.merge(top_posts_comments, posts_id_class, on=\"post_id\")\n",
        "\n",
        "comments = comments[comments['comment'].notna()]\n",
        "\n",
        "comments.sample(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Nff5WjuRCJJb",
        "outputId": "afc97d60-16a9-4857-92ad-c0f5f4737177"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        post_id                                            comment  \\\n",
              "111447   q2wb8u  work a side hustle personal project that you u...   \n",
              "115471   32ihpe  The question was specifically about newer work...   \n",
              "84442   10rx6tv                                     yes you should   \n",
              "104944   v9xme1  These folks probably worked for mbb and the li...   \n",
              "194090  1174kud  Not part of this conversation but - There is n...   \n",
              "\n",
              "              subreddit  \n",
              "111447      datascience  \n",
              "115471  MachineLearning  \n",
              "84442       datascience  \n",
              "104944      datascience  \n",
              "194090       artificial  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cd00b2fb-6a0a-41da-813e-ea6ad4e98a77\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post_id</th>\n",
              "      <th>comment</th>\n",
              "      <th>subreddit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>111447</th>\n",
              "      <td>q2wb8u</td>\n",
              "      <td>work a side hustle personal project that you u...</td>\n",
              "      <td>datascience</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115471</th>\n",
              "      <td>32ihpe</td>\n",
              "      <td>The question was specifically about newer work...</td>\n",
              "      <td>MachineLearning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84442</th>\n",
              "      <td>10rx6tv</td>\n",
              "      <td>yes you should</td>\n",
              "      <td>datascience</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104944</th>\n",
              "      <td>v9xme1</td>\n",
              "      <td>These folks probably worked for mbb and the li...</td>\n",
              "      <td>datascience</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>194090</th>\n",
              "      <td>1174kud</td>\n",
              "      <td>Not part of this conversation but - There is n...</td>\n",
              "      <td>artificial</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cd00b2fb-6a0a-41da-813e-ea6ad4e98a77')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cd00b2fb-6a0a-41da-813e-ea6ad4e98a77 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cd00b2fb-6a0a-41da-813e-ea6ad4e98a77');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform text pre-processing"
      ],
      "metadata": {
        "id": "gCzT2xkmJIsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reference 2 (Ref 2): https://www.analyticsvidhya.com/blog/2021/06/text-preprocessing-in-nlp-with-python-codes/\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('tagsets')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "nltk_stopwords = stopwords.words(\"english\")\n",
        "\n",
        "# nltk.help.upenn_tagset()\n",
        "\n",
        "# ---------- Lower-casing ----------\n",
        "# Ref 2 starts =====\n",
        "comments[\"comment\"] = comments[\"comment\"].apply(lambda c : c.lower())\n",
        "# ===== Ref 2 ends\n",
        "\n",
        "# ---------- Tokenization and Duplicate removal ----------\n",
        "# Ref 2 starts =====\n",
        "def tokenize(text):\n",
        "  tokens = re.split(\"\\W+\", text)\n",
        "# ===== Ref 2 ends\n",
        "  tokens = list(filter(None, tokens))\n",
        "  return sorted(list(set(tokens)))\n",
        "\n",
        "# Ref 2 starts =====\n",
        "comments[\"tokens\"] = comments[\"comment\"].apply(lambda c: tokenize(c))\n",
        "# ===== Ref 2 ends\n",
        "\n",
        "# ---------- Stopword removal ----------\n",
        "# Ref 2 starts =====\n",
        "def remove_stopwords(words):\n",
        "  output = [w for w in words if w not in nltk_stopwords]\n",
        "  return output\n",
        "# ===== Ref 2 ends\n",
        "\n",
        "# Ref 2 starts =====\n",
        "# comments[\"tokens\"] = comments[\"tokens\"].apply(lambda words: remove_stopwords(words))\n",
        "# ===== Ref 2 ends\n",
        "\n",
        "# ---------- Punctuation removal ----------\n",
        "# Ref 2 starts =====\n",
        "def remove_punc_str(text):\n",
        "  punc_free = \"\".join([char for char in text if char not in string.punctuation])\n",
        "  return punc_free\n",
        "# ===== Ref 2 ends\n",
        "\n",
        "def remove_punc_arr(words):\n",
        "  for i, w in enumerate(words):\n",
        "    # Ref 2 starts =====\n",
        "    punc_free = \"\".join([char for char in w if char not in string.punctuation])\n",
        "    words[i] = punc_free\n",
        "    # ===== Ref 2 ends\n",
        "  return words\n",
        "\n",
        "# Ref 2 starts =====\n",
        "comments[\"tokens\"] = comments[\"tokens\"].apply(lambda c: remove_punc_arr(c))\n",
        "# ===== Ref 2 ends\n",
        "\n",
        "# ---------- Lemmatization ----------\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def get_wordnet_pos(tag: str):\n",
        "  if tag.startswith(\"JJ\"):\n",
        "    return wordnet.ADJ\n",
        "  elif tag.startswith(\"NN\"):\n",
        "    return wordnet.NOUN\n",
        "  elif tag.startswith(\"RB\"):\n",
        "    return wordnet.ADV\n",
        "  elif tag.startswith(\"V\"):\n",
        "    return wordnet.VERB\n",
        "  else:\n",
        "    return wordnet.NOUN\n",
        "\n",
        "def lemmatize_tokens(tokens):\n",
        "  new_tokens = set()\n",
        "  for token, tag in nltk.pos_tag(tokens):\n",
        "    lemma = lemmatizer.lemmatize(token, get_wordnet_pos(tag))\n",
        "    new_tokens.add(lemma)\n",
        "  return list(new_tokens)\n",
        "\n",
        "comments[\"tokens\"] = comments[\"tokens\"].apply(lambda c: lemmatize_tokens(c))\n",
        "\n",
        "comments.sample(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "-NUkD0ldC2fp",
        "outputId": "8ae8eda3-8179-447d-e22b-8d84d135166b"
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Package tagsets is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       post_id                                            comment  \\\n",
              "218727  md1h0x  for real?! this thing is from 2017, that's so ...   \n",
              "174556  bjbrju  no, i’ve never used it. the 1 cycle policy was...   \n",
              "202278  wawec9            it’s the love cooking hate dishes thing   \n",
              "42311   70vuj5                                     it was edited.   \n",
              "5698    dh2xfs                                   link not opening   \n",
              "\n",
              "              subreddit                                             tokens  \n",
              "218727       artificial  [so, real, any, s, 2017, from, the, cool, idea...  \n",
              "174556  MachineLearning  [jeremy, write, fastai, never, think, use, bec...  \n",
              "202278      datascience        [hate, s, cook, the, dish, it, thing, love]  \n",
              "42311   MachineLearning                                     [it, edit, be]  \n",
              "5698    MachineLearning                                  [open, link, not]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3b090b36-6744-4f53-92a2-677d789048c7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post_id</th>\n",
              "      <th>comment</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>218727</th>\n",
              "      <td>md1h0x</td>\n",
              "      <td>for real?! this thing is from 2017, that's so ...</td>\n",
              "      <td>artificial</td>\n",
              "      <td>[so, real, any, s, 2017, from, the, cool, idea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174556</th>\n",
              "      <td>bjbrju</td>\n",
              "      <td>no, i’ve never used it. the 1 cycle policy was...</td>\n",
              "      <td>MachineLearning</td>\n",
              "      <td>[jeremy, write, fastai, never, think, use, bec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202278</th>\n",
              "      <td>wawec9</td>\n",
              "      <td>it’s the love cooking hate dishes thing</td>\n",
              "      <td>datascience</td>\n",
              "      <td>[hate, s, cook, the, dish, it, thing, love]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42311</th>\n",
              "      <td>70vuj5</td>\n",
              "      <td>it was edited.</td>\n",
              "      <td>MachineLearning</td>\n",
              "      <td>[it, edit, be]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5698</th>\n",
              "      <td>dh2xfs</td>\n",
              "      <td>link not opening</td>\n",
              "      <td>MachineLearning</td>\n",
              "      <td>[open, link, not]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3b090b36-6744-4f53-92a2-677d789048c7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3b090b36-6744-4f53-92a2-677d789048c7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3b090b36-6744-4f53-92a2-677d789048c7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 189
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split the Dataset"
      ],
      "metadata": {
        "id": "IMhAjZykXvlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reference 3 (Ref 3): https://stackoverflow.com/questions/43777243/how-to-split-a-dataframe-in-pandas-in-predefined-percentages \n",
        "\n",
        "# Ref 3 starts =====\n",
        "def split_by_fractions(df, fracs, random_state=0):\n",
        "    remain = df.index.copy().to_frame()\n",
        "    res = []\n",
        "    for i in range(len(fracs)):\n",
        "        fractions_sum = sum(fracs[i:])\n",
        "        frac = fracs[i]/fractions_sum\n",
        "        idxs = remain.sample(frac=frac, random_state=random_state).index\n",
        "        remain=remain.drop(idxs)\n",
        "        res.append(idxs)\n",
        "    return [df.loc[idxs] for idxs in res]\n",
        "# ===== Ref 3 ends\n",
        "\n",
        "random_state = 1\n",
        "train, dev, test = split_by_fractions(comments, [0.7, 0.2, 0.1], random_state)\n",
        "print(train.shape, dev.shape, test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9A_KRbSLNWq",
        "outputId": "f759c396-0ad2-47ea-fc0e-89fc0f3b7910"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(156211, 4) (44632, 4) (22316, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the classifier"
      ],
      "metadata": {
        "id": "SV5yn6POYikB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from decimal import Decimal\n",
        "\n",
        "train_ml = train.loc[train[\"subreddit\"] == \"MachineLearning\"]\n",
        "train_ds = train.loc[train[\"subreddit\"] == \"datascience\"]\n",
        "train_ai = train.loc[train[\"subreddit\"] == \"artificial\"]\n",
        "\n",
        "# P(ml), P(ds), and P(ai) priors\n",
        "p_ml = Decimal(len(train_ml) / len(train))\n",
        "p_ds = Decimal(len(train_ds) / len(train))\n",
        "p_ai = Decimal(len(train_ai) / len(train))\n",
        "\n",
        "print(f'P(ml) = {p_ml}')\n",
        "print(f'P(ds) = {p_ds}')\n",
        "print(f'P(ai) = {p_ai}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FN_YzCOuYZOZ",
        "outputId": "ad121510-7949-4cde-b12a-704c7d9a7873"
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P(ml) = 0.427972421916510359363172710800427012145519256591796875\n",
            "P(ds) = 0.4805679497602601824013390796608291566371917724609375\n",
            "P(ai) = 0.09145962832322947211327601735320058651268482208251953125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count word occurences\n",
        "\n",
        "occ_ml = {}\n",
        "occ_ds = {}\n",
        "occ_ai = {}\n",
        "\n",
        "def count_occurrences(occ_dict, df):\n",
        "  for words in df.loc[:,\"tokens\"]:\n",
        "    for w in words:\n",
        "      if w not in occ_dict.keys():\n",
        "        occ_dict[w] = 1\n",
        "      else:\n",
        "        occ_dict[w] += 1\n",
        "\n",
        "count_occurrences(occ_ml, train_ml)\n",
        "count_occurrences(occ_ds, train_ds)\n",
        "count_occurrences(occ_ai, train_ai)"
      ],
      "metadata": {
        "id": "XVcwU6ZymH5D"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate word probabilities given ml, ds, or ai\n",
        "\n",
        "probs_ml = {}\n",
        "probs_ds = {}\n",
        "probs_ai = {}\n",
        "\n",
        "def calc_word_likelihood(count, alpha, h):\n",
        "  if h == \"ml\":\n",
        "    return Decimal((count + alpha) / (len(train_ml) + alpha * 3))\n",
        "  elif h == \"ds\":\n",
        "    return Decimal((count + alpha) / (len(train_ds) + alpha * 3))\n",
        "  elif h == \"ai\":\n",
        "    return Decimal((count + alpha) / (len(train_ai) + alpha * 3))\n",
        "\n",
        "def calc_prob(alpha=0):\n",
        "  for word, count in occ_ml.items():\n",
        "    probs_ml[word] = calc_word_likelihood(count, alpha, \"ml\")\n",
        "  \n",
        "  for word, count in occ_ds.items():\n",
        "    probs_ds[word] = calc_word_likelihood(count, alpha, \"ds\")\n",
        "\n",
        "  for word, count in occ_ai.items():\n",
        "    probs_ai[word] = calc_word_likelihood(count, alpha, \"ai\")\n",
        "\n",
        "alpha = 1\n",
        "\n",
        "calc_prob(alpha)"
      ],
      "metadata": {
        "id": "oH2mTa61nIxe"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validate with ***dev*** dataset"
      ],
      "metadata": {
        "id": "UunmABI6pLq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import os\n",
        "\n",
        "# Function to classify a comment\n",
        "def classify(comment_words, alpha):\n",
        "  for w in comment_words:\n",
        "    if w not in probs_ml.keys():\n",
        "      probs_ml[w] = calc_word_likelihood(0, alpha, \"ml\")\n",
        "    if w not in probs_ds.keys():\n",
        "      probs_ds[w] = calc_word_likelihood(0, alpha, \"ds\")\n",
        "    if w not in probs_ai.keys():\n",
        "      probs_ai[w] = calc_word_likelihood(0, alpha, \"ai\")\n",
        "  \n",
        "  chance_ml = p_ml\n",
        "  chance_ds = p_ds\n",
        "  chance_ai = p_ai\n",
        "  for w in comment_words:\n",
        "    chance_ml = chance_ml * probs_ml[w]\n",
        "    chance_ds = chance_ds * probs_ds[w]\n",
        "    chance_ai = chance_ai * probs_ai[w]\n",
        "  \n",
        "  max_chance = max(chance_ml, chance_ds, chance_ai)\n",
        "\n",
        "  if max_chance == chance_ml:\n",
        "    return \"MachineLearning\"\n",
        "  elif max_chance == chance_ds:\n",
        "    return \"datascience\"\n",
        "  else:\n",
        "    return \"artificial\"\n",
        "    \n",
        "\n",
        "# Function to test entire dataset given\n",
        "def test_accuracy(dataset, alpha, csv_writer=None):\n",
        "  correct = 0\n",
        "  \n",
        "  for index, row in dataset.loc[:,[\"subreddit\", \"tokens\"]].iterrows():\n",
        "    result = classify(row[\"tokens\"], alpha)\n",
        "    if row[\"subreddit\"] == result:\n",
        "      correct += 1\n",
        "  \n",
        "  accuracy = round(correct / len(dataset) * 100, 4)\n",
        "\n",
        "  if csv_writer != None:\n",
        "    csv_writer.writerow([alpha, accuracy])\n",
        "  \n",
        "  print(f\"Successfully classified {correct}/{len(dataset)} ({accuracy}%) correctly\")\n",
        "\n",
        "# ---------- Experiment with Smoothing ----------\n",
        "\n",
        "# dev_smoothing = open(f\"dev_smoothing.csv\", \"a\", newline='')\n",
        "# dev_smoothing_writer = csv.writer(dev_smoothing)\n",
        "# if (os.path.getsize(f\"/content/dev_smoothing.csv\") == 0):\n",
        "#   dev_smoothing_writer.writerow([\"alpha\", \"accuracy\"])\n",
        "\n",
        "test_accuracy(dev, alpha)\n",
        "\n",
        "# dev_smoothing.flush()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0UcJ4gqpA_A",
        "outputId": "5b48a42f-02da-4e8c-d88b-2841bf84c092"
      },
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully classified 31520/44632 (70.622%) correctly\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclude final accuracy with ***test*** dataset"
      ],
      "metadata": {
        "id": "MpU_XG6osRoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test_smoothing = open(f'test_smoothing.csv', 'a', newline='')\n",
        "# test_smoothing_writer = csv.writer(test_smoothing)\n",
        "# if (os.path.getsize(f\"/content/test_smoothing.csv\") == 0):\n",
        "#   dev_smoothing_writer.writerow([\"alpha\", \"accuracy\"])\n",
        "\n",
        "test_accuracy(test, alpha)\n",
        "\n",
        "# test_smoothing.flush()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qh-hga8-sRQD",
        "outputId": "ec005429-d9f2-45ed-c859-5d0437f9cac8"
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully classified 15787/22316 (70.743%) correctly\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2WFadKPvto67"
      },
      "execution_count": 195,
      "outputs": []
    }
  ]
}
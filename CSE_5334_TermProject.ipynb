{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1UJen3sXTYGILgXlQ6vzCBqWOlaNcNAtb",
      "authorship_tag": "ABX9TyPYL6vm+RZHYWCAZjOZ3p6+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hayden-huynh/CSE-5334-TermProject/blob/master/CSE_5334_TermProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CSE 5334 - Data Mining: Term Project"
      ],
      "metadata": {
        "id": "spl--tTaQIVz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## About The Project\n",
        "- Dataset: [Top Reddit Posts and Comments](https://www.kaggle.com/datasets/tushar5harma/topredditcomments?select=Top_Posts.csv)\n",
        "- Goal: Given a comment of a post, classify which subreddit (Machine Learning, Artificial Intelligence, or Data Science) that it belongs to\n",
        "- Solution: Naive Bayes Classifier for Text Classification"
      ],
      "metadata": {
        "id": "1uncsqW5QwC9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Steps\n",
        "1. Download the Top Reddit Posts and Comments [dataset](https://www.kaggle.com/datasets/tushar5harma/topredditcomments?select=Top_Posts.csv) from Kaggle\n",
        "2. Based on the original dataset, formulate a dataframe consisting of a column of **comment text**, and a column of subreddit class\n",
        "3. Perform text pre-processing\n",
        "  - Lower-casing\n",
        "  - Punctuation removal\n",
        "  - Tokenization and duplicate word removal\n",
        "4. Split the data into ***train*** (70%), ***dev*** (20%), and ***test*** (10%) subsets\n",
        "5. Train the Naive Bayes Classifier\n",
        "6. Experiment for optimization with **dev** dataset\n",
        "  - Smoothing\n",
        "  - Stopword removal\n",
        "  - Stemming / Lemmatization\n",
        "7. Conclude final accuracy with **test** dataset"
      ],
      "metadata": {
        "id": "tuZKsbctQNDX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the Dataset"
      ],
      "metadata": {
        "id": "9ryhN4-zRi99"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "4lJIzW-FP5OE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6535dbaf-867c-4f0b-997a-8c6a7d791f32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.13)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.27.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.65.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.26.15)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2022.12.7)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (2.0.12)\n",
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "topredditcomments.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  topredditcomments.zip\n",
            "replace Top_Posts.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: Top_Posts.csv           \n",
            "replace Top_Posts_Comments.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: Top_Posts_Comments.csv  \n"
          ]
        }
      ],
      "source": [
        "# Download the Top Reddit Posts & Reviews dataset from Kaggle\n",
        "# Reference 1 (Ref 1): https://www.analyticsvidhya.com/blog/2021/06/how-to-load-kaggle-datasets-directly-into-google-colab/\n",
        "\n",
        "# Ref 1 starts =====\n",
        "! pip install kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download tushar5harma/topredditcomments\n",
        "! unzip topredditcomments.zip\n",
        "# ===== Ref 1 ends"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Formulate Comment-Subreddit Dataframe"
      ],
      "metadata": {
        "id": "i03Qyh_9CakO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "top_posts = pd.read_csv(\"/content/Top_Posts.csv\")\n",
        "top_posts_comments = pd.read_csv(\"/content/Top_Posts_Comments.csv\")\n",
        "\n",
        "posts_id_class = top_posts[[\"post_id\", \"subreddit\"]]\n",
        "\n",
        "# Three classes: \"MachineLearning\", \"datascience\", \"artificial\"\n",
        "comments = pd.merge(top_posts_comments, posts_id_class, on=\"post_id\")\n",
        "\n",
        "comments = comments[comments['comment'].notna()]\n",
        "\n",
        "comments.sample(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Nff5WjuRCJJb",
        "outputId": "2363c403-0e84-4a44-c961-3017a9791f90"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       post_id                                            comment  \\\n",
              "205581  ttfo0c       Or you are working in predictive maintenance   \n",
              "44187   hkiyir  \\> They cite to the SILU paper, which was publ...   \n",
              "30430   8rdpwy  Jesus, where to begin. It's a performance metr...   \n",
              "222650  dt628c  It’s how all “dangerous” ML techniques will go...   \n",
              "230     kuc6tz  Agree - it was 6 years later until MNIST was e...   \n",
              "\n",
              "              subreddit  \n",
              "205581      datascience  \n",
              "44187   MachineLearning  \n",
              "30430   MachineLearning  \n",
              "222650       artificial  \n",
              "230     MachineLearning  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a2d00d29-ebbb-4826-865f-8117a43a0067\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post_id</th>\n",
              "      <th>comment</th>\n",
              "      <th>subreddit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>205581</th>\n",
              "      <td>ttfo0c</td>\n",
              "      <td>Or you are working in predictive maintenance</td>\n",
              "      <td>datascience</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44187</th>\n",
              "      <td>hkiyir</td>\n",
              "      <td>\\&gt; They cite to the SILU paper, which was publ...</td>\n",
              "      <td>MachineLearning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30430</th>\n",
              "      <td>8rdpwy</td>\n",
              "      <td>Jesus, where to begin. It's a performance metr...</td>\n",
              "      <td>MachineLearning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>222650</th>\n",
              "      <td>dt628c</td>\n",
              "      <td>It’s how all “dangerous” ML techniques will go...</td>\n",
              "      <td>artificial</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>230</th>\n",
              "      <td>kuc6tz</td>\n",
              "      <td>Agree - it was 6 years later until MNIST was e...</td>\n",
              "      <td>MachineLearning</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a2d00d29-ebbb-4826-865f-8117a43a0067')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a2d00d29-ebbb-4826-865f-8117a43a0067 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a2d00d29-ebbb-4826-865f-8117a43a0067');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform text pre-processing"
      ],
      "metadata": {
        "id": "gCzT2xkmJIsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reference 2 (Ref 2): https://www.analyticsvidhya.com/blog/2021/06/text-preprocessing-in-nlp-with-python-codes/\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk_stopwords = stopwords.words(\"english\")\n",
        "\n",
        "# ---------- Lower-casing ----------\n",
        "# Ref 2 starts =====\n",
        "comments[\"comment\"] = comments[\"comment\"].apply(lambda c : c.lower())\n",
        "# ===== Ref 2 ends\n",
        "\n",
        "# ---------- Tokenization and Duplicate removal ----------\n",
        "# Ref 2 starts =====\n",
        "def tokenize(text):\n",
        "  tokens = re.split(\"\\W+\", text)\n",
        "# ===== Ref 2 ends\n",
        "  tokens = list(filter(None, tokens))\n",
        "  return sorted(list(set(tokens)))\n",
        "\n",
        "# Ref 2 starts =====\n",
        "comments[\"tokens\"] = comments[\"comment\"].apply(lambda c: tokenize(c))\n",
        "# ===== Ref 2 ends\n",
        "\n",
        "# ---------- Stopword removal ----------\n",
        "# Ref 2 starts =====\n",
        "def remove_stopwords(words):\n",
        "  output = [w for w in words if w not in nltk_stopwords]\n",
        "  return output\n",
        "# ===== Ref 2 ends\n",
        "\n",
        "# Ref 2 starts =====\n",
        "comments[\"tokens\"] = comments[\"tokens\"].apply(lambda words: remove_stopwords(words))\n",
        "# ===== Ref 2 ends\n",
        "\n",
        "# ---------- Punctuation removal ----------\n",
        "# Ref 2 starts =====\n",
        "def remove_punc_str(text):\n",
        "  punc_free = \"\".join([char for char in text if char not in string.punctuation])\n",
        "  return punc_free\n",
        "# ===== Ref 2 ends\n",
        "\n",
        "def remove_punc_arr(words):\n",
        "  for i, w in enumerate(words):\n",
        "    # Ref 2 starts =====\n",
        "    punc_free = \"\".join([char for char in w if char not in string.punctuation])\n",
        "    words[i] = punc_free\n",
        "    # ===== Ref 2 ends\n",
        "  return words\n",
        "\n",
        "# Ref 2 starts =====\n",
        "comments[\"tokens\"] = comments[\"tokens\"].apply(lambda c: remove_punc_arr(c))\n",
        "# ===== Ref 2 ends\n",
        "\n",
        "comments.sample(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "-NUkD0ldC2fp",
        "outputId": "344faa33-ab67-4595-f4a5-b23b3299ba27"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       post_id                                            comment  \\\n",
              "85216   jk9uy4              good luck! i hope you find something.   \n",
              "149733  3eu2rv  this is brilliant! forget the ml, i need to up...   \n",
              "140792  fq2k7w  i am in that camp. i do still use python but m...   \n",
              "131137  tvp0nd  it goes both ways. candidates like me probably...   \n",
              "52457   kagp2b  almost no states has cases at that level. you ...   \n",
              "\n",
              "              subreddit                                             tokens  \n",
              "85216       datascience                [find, good, hope, luck, something]  \n",
              "149733  MachineLearning            [brilliant, d3, forget, game, ml, need]  \n",
              "140792      datascience  [analysis, better, camp, course, data, drawing...  \n",
              "131137      datascience  [1, 2, actually, afford, application, better, ...  \n",
              "52457       datascience      [almost, cases, idea, level, states, talking]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-925f0ca8-e91a-43ff-95d7-83d8ff704be9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post_id</th>\n",
              "      <th>comment</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>85216</th>\n",
              "      <td>jk9uy4</td>\n",
              "      <td>good luck! i hope you find something.</td>\n",
              "      <td>datascience</td>\n",
              "      <td>[find, good, hope, luck, something]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149733</th>\n",
              "      <td>3eu2rv</td>\n",
              "      <td>this is brilliant! forget the ml, i need to up...</td>\n",
              "      <td>MachineLearning</td>\n",
              "      <td>[brilliant, d3, forget, game, ml, need]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140792</th>\n",
              "      <td>fq2k7w</td>\n",
              "      <td>i am in that camp. i do still use python but m...</td>\n",
              "      <td>datascience</td>\n",
              "      <td>[analysis, better, camp, course, data, drawing...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131137</th>\n",
              "      <td>tvp0nd</td>\n",
              "      <td>it goes both ways. candidates like me probably...</td>\n",
              "      <td>datascience</td>\n",
              "      <td>[1, 2, actually, afford, application, better, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52457</th>\n",
              "      <td>kagp2b</td>\n",
              "      <td>almost no states has cases at that level. you ...</td>\n",
              "      <td>datascience</td>\n",
              "      <td>[almost, cases, idea, level, states, talking]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-925f0ca8-e91a-43ff-95d7-83d8ff704be9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-925f0ca8-e91a-43ff-95d7-83d8ff704be9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-925f0ca8-e91a-43ff-95d7-83d8ff704be9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split the Dataset"
      ],
      "metadata": {
        "id": "IMhAjZykXvlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reference 3 (Ref 3): https://stackoverflow.com/questions/43777243/how-to-split-a-dataframe-in-pandas-in-predefined-percentages \n",
        "\n",
        "# Ref 3 starts =====\n",
        "def split_by_fractions(df, fracs, random_state=0):\n",
        "    remain = df.index.copy().to_frame()\n",
        "    res = []\n",
        "    for i in range(len(fracs)):\n",
        "        fractions_sum = sum(fracs[i:])\n",
        "        frac = fracs[i]/fractions_sum\n",
        "        idxs = remain.sample(frac=frac, random_state=random_state).index\n",
        "        remain=remain.drop(idxs)\n",
        "        res.append(idxs)\n",
        "    return [df.loc[idxs] for idxs in res]\n",
        "# ===== Ref 3 ends\n",
        "\n",
        "random_state = 1\n",
        "train, dev, test = split_by_fractions(comments, [0.7, 0.2, 0.1], random_state)\n",
        "print(train.shape, dev.shape, test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9A_KRbSLNWq",
        "outputId": "309307e3-3ada-42a7-f03b-71ad035b2018"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(156211, 4) (44632, 4) (22316, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the classifier"
      ],
      "metadata": {
        "id": "SV5yn6POYikB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from decimal import Decimal\n",
        "\n",
        "train_ml = train.loc[train[\"subreddit\"] == \"MachineLearning\"]\n",
        "train_ds = train.loc[train[\"subreddit\"] == \"datascience\"]\n",
        "train_ai = train.loc[train[\"subreddit\"] == \"artificial\"]\n",
        "\n",
        "# P(ml), P(ds), and P(ai) priors\n",
        "p_ml = Decimal(len(train_ml) / len(train))\n",
        "p_ds = Decimal(len(train_ds) / len(train))\n",
        "p_ai = Decimal(len(train_ai) / len(train))\n",
        "\n",
        "print(f'P(ml) = {p_ml}')\n",
        "print(f'P(ds) = {p_ds}')\n",
        "print(f'P(ai) = {p_ai}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FN_YzCOuYZOZ",
        "outputId": "06f1b603-6b05-4290-f39f-25be38591af0"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P(ml) = 0.427972421916510359363172710800427012145519256591796875\n",
            "P(ds) = 0.4805679497602601824013390796608291566371917724609375\n",
            "P(ai) = 0.09145962832322947211327601735320058651268482208251953125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count word occurences\n",
        "\n",
        "occ_ml = {}\n",
        "occ_ds = {}\n",
        "occ_ai = {}\n",
        "\n",
        "def count_occurrences(occ_dict, df):\n",
        "  for words in df.loc[:,\"tokens\"]:\n",
        "    for w in words:\n",
        "      if w not in occ_dict.keys():\n",
        "        occ_dict[w] = 1\n",
        "      else:\n",
        "        occ_dict[w] += 1\n",
        "\n",
        "count_occurrences(occ_ml, train_ml)\n",
        "count_occurrences(occ_ds, train_ds)\n",
        "count_occurrences(occ_ai, train_ai)"
      ],
      "metadata": {
        "id": "XVcwU6ZymH5D"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate word probabilities given ml, ds, or ai\n",
        "\n",
        "probs_ml = {}\n",
        "probs_ds = {}\n",
        "probs_ai = {}\n",
        "\n",
        "def calc_word_likelihood(count, alpha, h):\n",
        "  if h == \"ml\":\n",
        "    return Decimal((count + alpha) / (len(train_ml) + alpha * 3))\n",
        "  elif h == \"ds\":\n",
        "    return Decimal((count + alpha) / (len(train_ds) + alpha * 3))\n",
        "  elif h == \"ai\":\n",
        "    return Decimal((count + alpha) / (len(train_ai) + alpha * 3))\n",
        "\n",
        "def calc_prob(alpha=0):\n",
        "  for word, count in occ_ml.items():\n",
        "    probs_ml[word] = calc_word_likelihood(count, alpha, \"ml\")\n",
        "  \n",
        "  for word, count in occ_ds.items():\n",
        "    probs_ds[word] = calc_word_likelihood(count, alpha, \"ds\")\n",
        "\n",
        "  for word, count in occ_ai.items():\n",
        "    probs_ai[word] = calc_word_likelihood(count, alpha, \"ai\")\n",
        "\n",
        "alpha = 1\n",
        "\n",
        "calc_prob(alpha)"
      ],
      "metadata": {
        "id": "oH2mTa61nIxe"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validate with ***dev*** dataset"
      ],
      "metadata": {
        "id": "UunmABI6pLq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import os\n",
        "\n",
        "# Function to classify a comment\n",
        "def classify(comment_words, alpha):\n",
        "  for w in comment_words:\n",
        "    if w not in probs_ml.keys():\n",
        "      probs_ml[w] = calc_word_likelihood(0, alpha, \"ml\")\n",
        "    if w not in probs_ds.keys():\n",
        "      probs_ds[w] = calc_word_likelihood(0, alpha, \"ds\")\n",
        "    if w not in probs_ai.keys():\n",
        "      probs_ai[w] = calc_word_likelihood(0, alpha, \"ai\")\n",
        "  \n",
        "  chance_ml = p_ml\n",
        "  chance_ds = p_ds\n",
        "  chance_ai = p_ai\n",
        "  for w in comment_words:\n",
        "    chance_ml = chance_ml * probs_ml[w]\n",
        "    chance_ds = chance_ds * probs_ds[w]\n",
        "    chance_ai = chance_ai * probs_ai[w]\n",
        "  \n",
        "  max_chance = max(chance_ml, chance_ds, chance_ai)\n",
        "\n",
        "  if max_chance == chance_ml:\n",
        "    return \"MachineLearning\"\n",
        "  elif max_chance == chance_ds:\n",
        "    return \"datascience\"\n",
        "  else:\n",
        "    return \"artificial\"\n",
        "    \n",
        "\n",
        "# Function to test entire dataset given\n",
        "def test_accuracy(dataset, alpha, csv_writer=None):\n",
        "  correct = 0\n",
        "  \n",
        "  for index, row in dataset.loc[:,[\"subreddit\", \"tokens\"]].iterrows():\n",
        "    result = classify(row[\"tokens\"], alpha)\n",
        "    if row[\"subreddit\"] == result:\n",
        "      correct += 1\n",
        "  \n",
        "  accuracy = round(correct / len(dataset) * 100, 4)\n",
        "\n",
        "  if csv_writer != None:\n",
        "    csv_writer.writerow([alpha, accuracy])\n",
        "  \n",
        "  print(f\"Successfully classified {correct}/{len(dataset)} ({accuracy}%) correctly\")\n",
        "\n",
        "# ---------- Experiment with Smoothing ----------\n",
        "\n",
        "# dev_smoothing = open(f\"dev_smoothing.csv\", \"a\", newline='')\n",
        "# dev_smoothing_writer = csv.writer(dev_smoothing)\n",
        "# if (os.path.getsize(f\"/content/dev_smoothing.csv\") == 0):\n",
        "#   dev_smoothing_writer.writerow([\"alpha\", \"accuracy\"])\n",
        "\n",
        "test_accuracy(dev, alpha)\n",
        "\n",
        "# dev_smoothing.flush()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0UcJ4gqpA_A",
        "outputId": "a5b54ce8-0d69-45c9-8602-cc33ca065fe6"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully classified 31404/44632 (70.3621%) correctly\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclude final accuracy with ***test*** dataset"
      ],
      "metadata": {
        "id": "MpU_XG6osRoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test_smoothing = open(f'test_smoothing.csv', 'a', newline='')\n",
        "# test_smoothing_writer = csv.writer(test_smoothing)\n",
        "# if (os.path.getsize(f\"/content/test_smoothing.csv\") == 0):\n",
        "#   dev_smoothing_writer.writerow([\"alpha\", \"accuracy\"])\n",
        "\n",
        "test_accuracy(test, alpha)\n",
        "\n",
        "# test_smoothing.flush()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qh-hga8-sRQD",
        "outputId": "c8043953-6f6f-4c22-bf19-4338bcca2e19"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully classified 15663/22316 (70.1873%) correctly\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2WFadKPvto67"
      },
      "execution_count": 174,
      "outputs": []
    }
  ]
}